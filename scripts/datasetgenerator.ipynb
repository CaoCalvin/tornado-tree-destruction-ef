{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc925d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 15:21:40,042 - INFO - Using interpolation method: INTER_AREA\n",
      "2025-05-29 15:21:40,044 - INFO - Last processed timestamp: N/A (full run)\n",
      "2025-05-29 15:21:40,045 - INFO - Parsing CVAT XML: ..\\dataset\\project_combined.xml\n",
      "2025-05-29 15:21:40,131 - INFO - Parsed 36 image entries from XML.\n",
      "2025-05-29 15:21:40,132 - INFO - Processing large image: ..\\dataset\\crozier\\22_Crozier_461000_5385000.tif\n",
      "2025-05-29 15:21:41,880 - INFO - Generating full mask for 22_Crozier_461000_5385000.tif...\n",
      "2025-05-29 15:22:02,829 - INFO - Saved 1311 tiles for 22_Crozier_461000_5385000.tif.\n",
      "2025-05-29 15:22:02,874 - INFO - Processing large image: ..\\dataset\\dugwal\\22_Dugwal_499000_5383000.tif\n",
      "2025-05-29 15:22:04,666 - INFO - Generating full mask for 22_Dugwal_499000_5383000.tif...\n",
      "2025-05-29 15:22:07,098 - INFO - Saved 137 tiles for 22_Dugwal_499000_5383000.tif.\n",
      "2025-05-29 15:22:07,131 - INFO - Processing large image: ..\\dataset\\dugwal\\22_Dugwal_499000_5384000.tif\n",
      "2025-05-29 15:22:08,656 - INFO - Generating full mask for 22_Dugwal_499000_5384000.tif...\n",
      "2025-05-29 15:22:11,221 - INFO - Saved 108 tiles for 22_Dugwal_499000_5384000.tif.\n",
      "2025-05-29 15:22:11,272 - INFO - Processing large image: ..\\dataset\\dugwal\\22_Dugwal_500000_5381000.tif\n",
      "2025-05-29 15:22:13,191 - INFO - Generating full mask for 22_Dugwal_500000_5381000.tif...\n",
      "2025-05-29 15:22:13,656 - INFO - Saved 11 tiles for 22_Dugwal_500000_5381000.tif.\n",
      "2025-05-29 15:22:13,714 - INFO - Processing large image: ..\\dataset\\dugwal\\22_Dugwal_500000_5382000.tif\n",
      "2025-05-29 15:22:15,514 - INFO - Generating full mask for 22_Dugwal_500000_5382000.tif...\n",
      "2025-05-29 15:22:19,460 - INFO - Saved 193 tiles for 22_Dugwal_500000_5382000.tif.\n",
      "2025-05-29 15:22:19,502 - INFO - Processing large image: ..\\dataset\\dugwal\\22_Dugwal_500000_5383000.tif\n",
      "2025-05-29 15:22:21,041 - INFO - Generating full mask for 22_Dugwal_500000_5383000.tif...\n",
      "2025-05-29 15:22:22,924 - INFO - Saved 113 tiles for 22_Dugwal_500000_5383000.tif.\n",
      "2025-05-29 15:22:22,956 - INFO - Processing large image: ..\\dataset\\dugwal\\22_Dugwal_501000_5381000.tif\n",
      "2025-05-29 15:22:24,592 - INFO - Generating full mask for 22_Dugwal_501000_5381000.tif...\n",
      "2025-05-29 15:22:31,269 - INFO - Saved 411 tiles for 22_Dugwal_501000_5381000.tif.\n",
      "2025-05-29 15:22:31,311 - INFO - Processing large image: ..\\dataset\\dugwal\\22_Dugwal_501000_5382000.tif\n",
      "2025-05-29 15:22:32,795 - INFO - Generating full mask for 22_Dugwal_501000_5382000.tif...\n",
      "2025-05-29 15:22:32,968 - INFO - Saved 7 tiles for 22_Dugwal_501000_5382000.tif.\n",
      "2025-05-29 15:22:33,000 - INFO - Processing large image: ..\\dataset\\dugwal\\22_Dugwal_502000_5380000.tif\n",
      "2025-05-29 15:22:34,592 - INFO - Generating full mask for 22_Dugwal_502000_5380000.tif...\n",
      "2025-05-29 15:22:39,294 - INFO - Saved 310 tiles for 22_Dugwal_502000_5380000.tif.\n",
      "2025-05-29 15:22:39,324 - INFO - Processing large image: ..\\dataset\\dugwal\\22_Dugwal_503000_5379000.tif\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# --- Configuration ---\n",
    "OUTPUT_TILE_SIZE = 512\n",
    "STRIDE = 256\n",
    "DOWNSCALE_THRESHOLD = 10000\n",
    "DOWNSCALE_FACTOR = 2.0 # Scale factor (e.g., 2.0 means halve the size)\n",
    "\n",
    "CLASS_MAPPING = {\n",
    "    'upright': 0,\n",
    "    'fallen': 1,\n",
    "    'other': 2,\n",
    "    'unlabeled': 3\n",
    "}\n",
    "# 'incomplete' is an error class, not mapped to a value.\n",
    "\n",
    "# Priority for drawing when classes overlap at the same z-level (higher value = higher priority)\n",
    "CLASS_PRIORITY = {\n",
    "    'fallen': 3,\n",
    "    'other': 2,\n",
    "    'upright': 1, # Explicit upright polygons\n",
    "    'unlabeled': 0\n",
    "}\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def parse_points_string(points_str, scale_factor=1.0):\n",
    "    \"\"\"Parses a string of points 'x1,y1;x2,y2;...' into a list of [x,y] tuples and scales them.\"\"\"\n",
    "    points = []\n",
    "    try:\n",
    "        for p_pair in points_str.split(';'):\n",
    "            if not p_pair: continue\n",
    "            coords = p_pair.split(',')\n",
    "            x = float(coords[0]) * scale_factor\n",
    "            y = float(coords[1]) * scale_factor\n",
    "            points.append([x, y])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing points string '{points_str[:50]}...': {e}\")\n",
    "        return None\n",
    "    return np.array(points, dtype=np.int32)\n",
    "\n",
    "def read_timestamp(file_path):\n",
    "    \"\"\"Reads a timestamp from a file.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            try:\n",
    "                return float(f.read().strip())\n",
    "            except ValueError:\n",
    "                return 0.0\n",
    "    return 0.0\n",
    "\n",
    "def write_timestamp(file_path, timestamp):\n",
    "    \"\"\"Writes a timestamp to a file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(str(timestamp))\n",
    "\n",
    "def get_file_mod_time(file_path):\n",
    "    \"\"\"Gets the modification time of a file.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        return os.path.getmtime(file_path)\n",
    "    return 0.0\n",
    "\n",
    "def generate_full_mask(image_dims, polygons_data, class_mapping, class_priority):\n",
    "    \"\"\"\n",
    "    Generates a full-size mask for a large image based on polygon annotations.\n",
    "    polygons_data: list of dicts {'points': np.array, 'label': str, 'z_order': int}\n",
    "    \"\"\"\n",
    "    height, width = image_dims\n",
    "    full_mask = np.full((height, width), class_mapping['upright'], dtype=np.uint8)\n",
    "\n",
    "    if not polygons_data:\n",
    "        return full_mask\n",
    "\n",
    "    # Sort polygons first by z-order (ascending), then by class priority (descending for drawing order)\n",
    "    # This means lower z-orders are processed first. For a given z-order, higher priority classes are drawn last (on top).\n",
    "    \n",
    "    # Get all unique z-orders\n",
    "    z_orders = sorted(list(set(p['z_order'] for p in polygons_data)))\n",
    "\n",
    "    for z in z_orders:\n",
    "        polys_at_z = [p for p in polygons_data if p['z_order'] == z]\n",
    "        \n",
    "        # Sort polygons at the current z-level by class priority (ascending, so higher priority drawn last)\n",
    "        polys_at_z_sorted = sorted(polys_at_z, key=lambda p: class_priority.get(p['label'], -1))\n",
    "\n",
    "        for poly_info in polys_at_z_sorted:\n",
    "            label = poly_info['label']\n",
    "            points = poly_info['points']\n",
    "            \n",
    "            if label in class_mapping and points is not None and len(points) > 0:\n",
    "                cv_points = points.reshape((-1, 1, 2)) # OpenCV format\n",
    "                cv2.fillPoly(full_mask, [cv_points], class_mapping[label])\n",
    "            elif label not in class_mapping and label != 'incomplete':\n",
    "                 logging.warning(f\"Unknown label '{label}' encountered in polygon data. Skipping this polygon.\")\n",
    "\n",
    "\n",
    "    return full_mask\n",
    "\n",
    "def process_large_image(image_info, task_name, cvat_xml_path, input_image_root_dir,\n",
    "                        output_dataset_dir, last_proc_timestamp,\n",
    "                        interpolation_method=cv2.INTER_AREA):\n",
    "    \"\"\"\n",
    "    Processes a single large satellite image: loads, scales, generates mask, tiles, and saves.\n",
    "    image_info: dict from parsed XML containing {'id', 'name', 'width', 'height', 'task_id', 'polygons'}\n",
    "    polygons: list of {'label', 'points_str', 'z_order'}\n",
    "    \"\"\"\n",
    "    image_name = image_info['name']\n",
    "    image_path = os.path.join(input_image_root_dir, task_name, image_name)\n",
    "    xml_mod_time = get_file_mod_time(cvat_xml_path)\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        logging.warning(f\"Image file not found: {image_path}. Skipping.\")\n",
    "        return False # Indicate that no processing was done for timestamp update\n",
    "\n",
    "    image_mod_time = get_file_mod_time(image_path)\n",
    "\n",
    "    if image_mod_time <= last_proc_timestamp and xml_mod_time <= last_proc_timestamp :\n",
    "        logging.info(f\"Image {image_name} and XML are older than last processing time. Skipping.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "    logging.info(f\"Processing large image: {image_path}\")\n",
    "\n",
    "    # --- 1. Error check based on labels for the entire image ---\n",
    "    raw_labels = [p['label'] for p in image_info['polygons']]\n",
    "    if not raw_labels and not image_info['polygons']: # No polygons at all\n",
    "        logging.info(f\"Image {image_name} has no polygon annotations. It will likely result in empty tiles only.\")\n",
    "        # This isn't an error per se, tiles will just be skipped if all upright.\n",
    "    elif 'incomplete' in raw_labels:\n",
    "        logging.error(f\"Image {image_name} contains 'incomplete' labels. Skipping this image.\")\n",
    "        return True # Processed (checked), but skipped due to error\n",
    "    \n",
    "    # Check if only 'unlabeled' polygons exist (potentially with background)\n",
    "    # This means every polygon MUST be 'unlabeled' if polygons exist.\n",
    "    # If there are other types of labels, this condition is false.\n",
    "    if image_info['polygons'] and all(label == 'unlabeled' for label in raw_labels):\n",
    "        logging.error(f\"Image {image_name} contains only 'unlabeled' polygons. Skipping this image.\")\n",
    "        return True # Processed (checked), but skipped due to error\n",
    "\n",
    "    # --- 2. Load and potentially downscale image ---\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        logging.error(f\"Failed to load image: {image_path}. Skipping.\")\n",
    "        return True\n",
    "\n",
    "    original_height, original_width = img.shape[:2]\n",
    "    scale_factor = 1.0\n",
    "\n",
    "    if original_height > DOWNSCALE_THRESHOLD or original_width > DOWNSCALE_THRESHOLD:\n",
    "        logging.info(f\"Image {image_name} ({original_width}x{original_height}) exceeds threshold, downscaling by {DOWNSCALE_FACTOR}x.\")\n",
    "        scale_factor = 1.0 / DOWNSCALE_FACTOR\n",
    "        new_width = int(original_width * scale_factor)\n",
    "        new_height = int(original_height * scale_factor)\n",
    "        img = cv2.resize(img, (new_width, new_height), interpolation=interpolation_method)\n",
    "        logging.info(f\"Downscaled to {new_width}x{new_height}.\")\n",
    "    \n",
    "    current_height, current_width = img.shape[:2]\n",
    "\n",
    "    # --- 3. Prepare polygon data (parse points and scale) ---\n",
    "    scaled_polygons_data = []\n",
    "    for poly in image_info['polygons']:\n",
    "        parsed_pts = parse_points_string(poly['points_str'], scale_factor)\n",
    "        if parsed_pts is not None and len(parsed_pts) >=3 : # Polygons need at least 3 points\n",
    "             scaled_polygons_data.append({\n",
    "                'points': parsed_pts,\n",
    "                'label': poly['label'],\n",
    "                'z_order': poly['z_order']\n",
    "            })\n",
    "        elif parsed_pts is not None :\n",
    "            logging.warning(f\"Polygon with label '{poly['label']}' in {image_name} has < 3 points after scaling. Skipping polygon.\")\n",
    "\n",
    "\n",
    "    # --- 4. Generate full mask for the (potentially scaled) image ---\n",
    "    logging.info(f\"Generating full mask for {image_name}...\")\n",
    "    full_mask = generate_full_mask((current_height, current_width), scaled_polygons_data, CLASS_MAPPING, CLASS_PRIORITY)\n",
    "\n",
    "    # --- 5. Stride and save tiles ---\n",
    "    output_image_dir = os.path.join(output_dataset_dir, 'images', task_name)\n",
    "    output_mask_dir = os.path.join(output_dataset_dir, 'masks', task_name)\n",
    "    os.makedirs(output_image_dir, exist_ok=True)\n",
    "    os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "    tiles_saved_count = 0\n",
    "    for r in range(0, current_height - OUTPUT_TILE_SIZE + 1, STRIDE):\n",
    "        for c in range(0, current_width - OUTPUT_TILE_SIZE + 1, STRIDE):\n",
    "            img_tile = img[r:r+OUTPUT_TILE_SIZE, c:c+OUTPUT_TILE_SIZE]\n",
    "            mask_tile = full_mask[r:r+OUTPUT_TILE_SIZE, c:c+OUTPUT_TILE_SIZE]\n",
    "\n",
    "            # Skip if mask tile contains only the 'upright' class (background)\n",
    "            if np.all(mask_tile == CLASS_MAPPING['upright']):\n",
    "                continue\n",
    "\n",
    "            # Save tile\n",
    "            base_filename = f\"{os.path.splitext(image_name)[0]}_r{r}_c{c}\"\n",
    "            \n",
    "            img_tile_path = os.path.join(output_image_dir, f\"{base_filename}.png\")\n",
    "            cv2.imwrite(img_tile_path, img_tile)\n",
    "\n",
    "            mask_tile_path = os.path.join(output_mask_dir, f\"{base_filename}.png\")\n",
    "            cv2.imwrite(mask_tile_path, mask_tile)\n",
    "            tiles_saved_count += 1\n",
    "            \n",
    "    logging.info(f\"Saved {tiles_saved_count} tiles for {image_name}.\")\n",
    "    return True # Processed successfully\n",
    "\n",
    "def main(cvat_xml_path, input_image_root_dir, output_dir_base, interpolation_str=\"INTER_AREA\"):\n",
    "    \"\"\"Main processing function.\"\"\"\n",
    "\n",
    "    interpolation_methods = {\n",
    "        \"INTER_NEAREST\": cv2.INTER_NEAREST,\n",
    "        \"INTER_LINEAR\": cv2.INTER_LINEAR,\n",
    "        \"INTER_AREA\": cv2.INTER_AREA,\n",
    "        \"INTER_CUBIC\": cv2.INTER_CUBIC,\n",
    "        \"INTER_LANCZOS4\": cv2.INTER_LANCZOS4,\n",
    "    }\n",
    "    interpolation_method = interpolation_methods.get(interpolation_str, cv2.INTER_AREA)\n",
    "    logging.info(f\"Using interpolation method: {interpolation_str}\")\n",
    "\n",
    "    output_dataset_dir = os.path.join(output_dir_base, \"dataset\")\n",
    "    os.makedirs(output_dataset_dir, exist_ok=True)\n",
    "\n",
    "    timestamp_file = os.path.join(output_dataset_dir, \"lastmodified.txt\")\n",
    "    last_processed_timestamp = read_timestamp(timestamp_file)\n",
    "    logging.info(f\"Last processed timestamp: {datetime.fromtimestamp(last_processed_timestamp) if last_processed_timestamp > 0 else 'N/A (full run)'}\")\n",
    "\n",
    "    # --- Parse CVAT XML ---\n",
    "    logging.info(f\"Parsing CVAT XML: {cvat_xml_path}\")\n",
    "    try:\n",
    "        tree = ET.parse(cvat_xml_path)\n",
    "        root = tree.getroot()\n",
    "    except ET.ParseError as e:\n",
    "        logging.error(f\"Error parsing XML file: {e}\")\n",
    "        return\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"CVAT XML file not found: {cvat_xml_path}\")\n",
    "        return\n",
    "\n",
    "    # Create a mapping from task_id to task_name\n",
    "    task_id_to_name = {}\n",
    "    for task_elem in root.findall(\"./meta/project/tasks/task\"):\n",
    "        task_id = task_elem.find(\"id\").text\n",
    "        task_name = task_elem.find(\"name\").text\n",
    "        task_id_to_name[task_id] = task_name\n",
    "        # Ensure output directories for this task exist early\n",
    "        os.makedirs(os.path.join(output_dataset_dir, 'images', task_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dataset_dir, 'masks', task_name), exist_ok=True)\n",
    "\n",
    "\n",
    "    images_data = []\n",
    "    for image_elem in root.findall(\"./image\"):\n",
    "        img_id = image_elem.get(\"id\")\n",
    "        img_name = image_elem.get(\"name\")\n",
    "        img_width = int(image_elem.get(\"width\"))\n",
    "        img_height = int(image_elem.get(\"height\"))\n",
    "        task_id = image_elem.get(\"task_id\") # CVAT for images format\n",
    "        \n",
    "        # Fallback if task_id is not directly on image (older CVAT versions might have it on job)\n",
    "        if task_id is None: # Try to find task_id via job_id if structure implies\n",
    "            job_id = image_elem.get(\"job_id\") # Common in CVAT project exports\n",
    "            if job_id:\n",
    "                # This part is speculative without exact XML structure for job_id -> task_id mapping\n",
    "                # Assuming a simple structure or that task_id is preferred\n",
    "                logging.warning(f\"Image {img_name} missing direct task_id, has job_id {job_id}. Task mapping might be indirect.\")\n",
    "                # If you have a way to map job_id to task_id, implement here.\n",
    "                # For now, we rely on task_id on image element or image name convention.\n",
    "\n",
    "\n",
    "        # Determine task_name\n",
    "        current_task_name = task_id_to_name.get(task_id)\n",
    "        if not current_task_name:\n",
    "            # Try to infer task_name from image_name if convention like \"Location_...\" exists\n",
    "            # This is a fallback, relying on task_id is better.\n",
    "            parts = img_name.split('_')\n",
    "            if len(parts) > 1 and parts[0].isalpha(): # e.g. \"Crozier_...\"\n",
    "                 potential_task_name_from_img = parts[1] # Assuming \"ID_Location_...\"\n",
    "                 if potential_task_name_from_img in task_id_to_name.values():\n",
    "                      current_task_name = potential_task_name_from_img\n",
    "                      logging.warning(f\"Inferred task name '{current_task_name}' from image name for {img_name} as task_id was not directly mapped.\")\n",
    "                 else: # Try first part if that's the location\n",
    "                    potential_task_name_from_img = parts[0]\n",
    "                    if potential_task_name_from_img in task_id_to_name.values():\n",
    "                        current_task_name = potential_task_name_from_img\n",
    "                        logging.warning(f\"Inferred task name '{current_task_name}' (first part) from image name for {img_name} as task_id was not directly mapped.\")\n",
    "\n",
    "\n",
    "        if not current_task_name:\n",
    "            logging.error(f\"Could not determine task name for image {img_name} (task_id: {task_id}). Skipping this image.\")\n",
    "            continue\n",
    "            \n",
    "        polygons = []\n",
    "        for poly_elem in image_elem.findall(\"./polygon\"):\n",
    "            label = poly_elem.get(\"label\")\n",
    "            points_str = poly_elem.get(\"points\")\n",
    "            z_order = int(poly_elem.get(\"z_order\", \"0\"))\n",
    "            occluded = poly_elem.get(\"occluded\") == \"1\" # Example, if you need it\n",
    "\n",
    "            if label and points_str:\n",
    "                 polygons.append({'label': label, 'points_str': points_str, 'z_order': z_order})\n",
    "            else:\n",
    "                logging.warning(f\"Skipping polygon with missing label or points in image {img_name}\")\n",
    "\n",
    "        images_data.append({\n",
    "            'id': img_id, 'name': img_name, 'width': img_width, 'height': img_height,\n",
    "            'task_id': task_id, 'task_name': current_task_name, 'polygons': polygons\n",
    "        })\n",
    "    \n",
    "    logging.info(f\"Parsed {len(images_data)} image entries from XML.\")\n",
    "\n",
    "    something_processed = False\n",
    "    for image_info_dict in images_data:\n",
    "        task_name_for_image = image_info_dict['task_name']\n",
    "        if process_large_image(image_info_dict, task_name_for_image, cvat_xml_path,\n",
    "                               input_image_root_dir, output_dataset_dir,\n",
    "                               last_processed_timestamp, interpolation_method):\n",
    "            something_processed = True\n",
    "            \n",
    "    # Update timestamp if any image was processed or checked after the last timestamp\n",
    "    # Or if the XML itself is newer.\n",
    "    xml_mod_time = get_file_mod_time(cvat_xml_path)\n",
    "    if something_processed or xml_mod_time > last_processed_timestamp :\n",
    "        current_run_timestamp = datetime.now().timestamp()\n",
    "        write_timestamp(timestamp_file, current_run_timestamp)\n",
    "        logging.info(f\"Processing complete. Updated timestamp to: {datetime.fromtimestamp(current_run_timestamp)}\")\n",
    "    else:\n",
    "        logging.info(\"No new images or XML modifications found. Processing complete.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- User Configuration ---\n",
    "    # ! Adjust these paths according to your system !\n",
    "    CVAT_XML_FILE = r\"..\\dataset\\project_combined.xml\" # E.g., \"C:/cvat_exports/project_trees_annotations.xml\"\n",
    "    INPUT_IMAGE_ROOT = r\"..\\dataset\" # Base folder containing 'centreglassville', 'crozier', etc.\n",
    "    OUTPUT_DIRECTORY_BASE = r\"..\\dataset_processed\" # The 'dataset' folder will be created inside this directory\n",
    "    \n",
    "    # Optional: Change interpolation method string if needed.\n",
    "    # Options: \"INTER_NEAREST\", \"INTER_LINEAR\", \"INTER_AREA\", \"INTER_CUBIC\", \"INTER_LANCZOS4\"\n",
    "    SCALING_INTERPOLATION = \"INTER_AREA\" \n",
    "\n",
    "    # --- Run ---\n",
    "    # Example usage:\n",
    "    if CVAT_XML_FILE == \"path/to/your/annotations.xml\" or \\\n",
    "       INPUT_IMAGE_ROOT == \"C:/USERS/KEVIN/DEV/TORNADO-TREE-DESTRUCTION-EF/TORNADO-TREE-DESTRUCTION-EF/DATASET_NEEDS_UPDATE\" : # Quick check if paths were updated\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(\"!!! PLEASE UPDATE 'CVAT_XML_FILE' AND 'INPUT_IMAGE_ROOT' IN THE SCRIPT !!!\")\n",
    "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    else:\n",
    "        main(CVAT_XML_FILE, INPUT_IMAGE_ROOT, OUTPUT_DIRECTORY_BASE, SCALING_INTERPOLATION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-checked",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
