{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:29:59.376434Z",
     "iopub.status.busy": "2024-08-23T11:29:59.376015Z",
     "iopub.status.idle": "2024-08-23T11:30:06.026067Z",
     "shell.execute_reply": "2024-08-23T11:30:06.024970Z",
     "shell.execute_reply.started": "2024-08-23T11:29:59.376395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset as BaseDataset\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "import cv2 \n",
    "\n",
    "class Cls(Enum):\n",
    "    UPRIGHT = (0, \"#b7f2a6\")  # light green\n",
    "    FALLEN = (1, \"#c71933\")   # red\n",
    "    OTHER = (2, \"#ffcc33\")    # yellow\n",
    "\n",
    "    def __new__(cls, num, hex_color):\n",
    "        obj = object.__new__(cls)\n",
    "        obj._value_ = num\n",
    "        obj.hex_color = hex_color\n",
    "        return obj\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "base_path = os.path.join('..', 'dataset_processed')\n",
    "images_path = os.path.join(base_path, 'images')\n",
    "masks_path = os.path.join(base_path, 'masks')\n",
    "splits_path = os.path.join(base_path, 'splits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "Writing helper class for data extraction, tranformation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resplit(dataset_path, train_frac=0.7, val_frac=0.2, test_frac=0.1, seed=SEED):\n",
    "    assert train_frac + val_frac + test_frac == 1.0, \"Fractions must sum to 1.\"\n",
    "\n",
    "    scenes = sorted(os.listdir(os.path.join(dataset_path, 'images')))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(scenes)\n",
    "    \n",
    "    n = len(scenes)\n",
    "    n_train = int(n * train_frac)\n",
    "    # Use remaining scenes for both validation and test equally\n",
    "    remaining = n - n_train\n",
    "    half = remaining // 2  # ensure both splits have the same number of scenes\n",
    "    \n",
    "    train_scenes = scenes[:n_train]\n",
    "    val_scenes = scenes[n_train:n_train+half]\n",
    "    test_scenes = scenes[n_train+half:n_train+2*half]\n",
    "\n",
    "    os.makedirs(os.path.join(dataset_path, 'splits'), exist_ok=True)\n",
    "    for name, split in zip(['train', 'val', 'test'], [train_scenes, val_scenes, test_scenes]):\n",
    "        with open(os.path.join(dataset_path, 'splits', f'{name}.txt'), 'w') as f:\n",
    "            for scene in split:\n",
    "                f.write(f\"{scene}\\n\")\n",
    "\n",
    "class Dataset(BaseDataset):\n",
    "\n",
    "    \n",
    "    def __init__(self, image_root, mask_root, split_file, transform=None):\n",
    "        self.background_class = Cls.UPRIGHT.value\n",
    "        assert os.path.exists(image_root), f\"Image root {image_root} does not exist.\"\n",
    "        assert os.path.exists(mask_root), f\"Mask root {mask_root} does not exist.\"\n",
    "        assert os.path.exists(split_file), f\"Split file {split_file} does not exist.\"\n",
    "        \n",
    "        self.image_root = image_root\n",
    "        self.mask_root = mask_root\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(split_file, 'r') as f:\n",
    "            self.scenes = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        self.samples = []\n",
    "        for scene in self.scenes:\n",
    "            image_dir = os.path.join(self.image_root, scene)\n",
    "            mask_dir = os.path.join(self.mask_root, scene)\n",
    "            \n",
    "            for fname in sorted(os.listdir(image_dir)):\n",
    "                if not fname.endswith(('.png', '.jpg', '.jpeg', '.tiff', '.tif')):\n",
    "                    continue\n",
    "                img_path = os.path.join(image_dir, fname)\n",
    "                name, ext = os.path.splitext(fname)\n",
    "                mask_path = os.path.join(mask_dir, f\"{name}_mask{ext}\")  \n",
    "                self.samples.append((img_path, mask_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self.samples[idx]\n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "        return image, mask\n",
    "\n",
    "def visualize(**images):\n",
    "    \"\"\"Plot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(\" \".join(name.split(\"_\")).title())\n",
    "\n",
    "        # If it's an image, plot it as RGB\n",
    "        if name == \"image\":\n",
    "            # Convert CHW to HWC for plotting\n",
    "            image = image.transpose(1, 2, 0)\n",
    "            plt.imshow(image)\n",
    "        else:\n",
    "            plt.imshow(image, cmap=\"tab20\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:30:06.482545Z",
     "iopub.status.busy": "2024-08-23T11:30:06.482154Z",
     "iopub.status.idle": "2024-08-23T11:30:06.495170Z",
     "shell.execute_reply": "2024-08-23T11:30:06.494101Z",
     "shell.execute_reply.started": "2024-08-23T11:30:06.482503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=20, val_shift_limit=15, p=0.3),\n",
    "        \n",
    "        A.OneOf([\n",
    "            A.GaussianBlur(blur_limit=3, p=0.3),\n",
    "            A.MotionBlur(blur_limit=3, p=0.3),\n",
    "            A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.3),\n",
    "        ], p=0.3),\n",
    "        \n",
    "        A.GaussNoise(p=0.2),\n",
    "        \n",
    "        A.OneOf([\n",
    "            A.ElasticTransform(alpha=1, sigma=50, p=0.3, mask_interpolation=cv2.BORDER_CONSTANT, fill_mask=Cls.OTHER.value), \n",
    "            A.GridDistortion(num_steps=5, distort_limit=(-0.1, 0.1), p=0.3, mask_interpolation=cv2.BORDER_CONSTANT, fill_mask=Cls.OTHER.value),\n",
    "        ], p=0.2),\n",
    "        \n",
    "        A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.05, rotate_limit=0, \n",
    "                          p=0.3, mask_interpolation=cv2.BORDER_CONSTANT, fill_mask=Cls.OTHER.value),\n",
    "\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:30:06.039244Z",
     "iopub.status.busy": "2024-08-23T11:30:06.038840Z",
     "iopub.status.idle": "2024-08-23T11:30:06.054834Z",
     "shell.execute_reply": "2024-08-23T11:30:06.053760Z",
     "shell.execute_reply.started": "2024-08-23T11:30:06.039198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "resplit(base_path, train_frac=0.7, val_frac=0.15, test_frac=0.15)\n",
    "\n",
    "dataset_train = Dataset(\n",
    "    image_root=images_path,\n",
    "    mask_root=masks_path,\n",
    "    split_file=os.path.join(splits_path, 'train.txt'),\n",
    "    transform=get_training_augmentation()  \n",
    ")\n",
    "\n",
    "dataset_val = Dataset(\n",
    "    image_root=images_path,\n",
    "    mask_root=masks_path,\n",
    "    split_file=os.path.join(splits_path, 'val.txt'),\n",
    "    transform=None  \n",
    ")\n",
    "\n",
    "dataset_test = Dataset(\n",
    "    image_root=images_path,\n",
    "    mask_root=masks_path,\n",
    "    split_file=os.path.join(splits_path, 'test.txt'),\n",
    "    transform=None  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:30:06.071933Z",
     "iopub.status.busy": "2024-08-23T11:30:06.071406Z",
     "iopub.status.idle": "2024-08-23T11:30:06.480787Z",
     "shell.execute_reply": "2024-08-23T11:30:06.479553Z",
     "shell.execute_reply.started": "2024-08-23T11:30:06.071890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def hex_to_rgb(hex_color):\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "# Map original values to output RGB colors using Cls enum\n",
    "color_mapping = {\n",
    "    Cls.UPRIGHT.value: hex_to_rgb(Cls.UPRIGHT.hex_color),\n",
    "    Cls.FALLEN.value: hex_to_rgb(Cls.FALLEN.hex_color),\n",
    "    Cls.OTHER.value: hex_to_rgb(Cls.OTHER.hex_color),\n",
    "}\n",
    "\n",
    "def mask_to_color(mask):\n",
    "    \"\"\"\n",
    "    Convert a 2D mask with greyscale values into an RGB image using the color mapping.\n",
    "    Unrecognized values will be set to black.\n",
    "    \"\"\"\n",
    "    h, w = mask.shape\n",
    "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for cat_val in np.unique(mask):\n",
    "        # If the category value is not recognized, default to black (0, 0, 0)\n",
    "        color = color_mapping.get(cat_val, (0, 0, 0))\n",
    "        color_mask[mask == cat_val] = color\n",
    "    return color_mask\n",
    "def visualize(**images):\n",
    "    \"\"\"Plot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, img) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(\" \".join(name.split(\"_\")).title())\n",
    "        if name == \"image\":\n",
    "            # Check if image is in CHW format; if so, convert it to HWC for plotting\n",
    "            if img.shape[0] == 3:\n",
    "                try:\n",
    "                    # If image is a torch tensor, use permute\n",
    "                    import torch\n",
    "                    if isinstance(img, torch.Tensor):\n",
    "                        img = img.permute(1, 2, 0).cpu().numpy()\n",
    "                    else:\n",
    "                        img = img.transpose(1, 2, 0)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            plt.imshow(img)\n",
    "        else:\n",
    "            # Convert greyscale mask to output color using the mapping\n",
    "            color_mask = mask_to_color(img)\n",
    "            plt.imshow(color_mask)\n",
    "    plt.show()\n",
    "\n",
    "idx = random.randint(0, len(dataset_train) - 1)\n",
    "image, mask = dataset_train[idx] \n",
    "print(f\"Showing image {idx} of {len(dataset_train)}\")\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "visualize(image=image, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:30:07.615277Z",
     "iopub.status.busy": "2024-08-23T11:30:07.614938Z",
     "iopub.status.idle": "2024-08-23T11:30:07.638455Z",
     "shell.execute_reply": "2024-08-23T11:30:07.637474Z",
     "shell.execute_reply.started": "2024-08-23T11:30:07.615245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Some training hyperparameters TODO tune\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 4 \n",
    "T_MAX = EPOCHS * len(dataset_train)\n",
    "OUT_CLASSES = len(Cls)\n",
    "\n",
    "class CamVidModel(pl.LightningModule):\n",
    "    def __init__(self, arch, encoder_name, in_channels, out_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch,\n",
    "            encoder_name=encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            classes=out_classes,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Preprocessing parameters for image normalization\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name)\n",
    "        self.number_of_classes = out_classes\n",
    "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n",
    "\n",
    "        # Loss function for multi-class segmentation\n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.MULTICLASS_MODE, from_logits=True)\n",
    "\n",
    "        # Step metrics tracking\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Normalize image\n",
    "        image = (image - self.mean) / self.std\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        image, mask = batch\n",
    "\n",
    "        # Ensure that image dimensions are correct\n",
    "        assert image.ndim == 4  # [batch_size, channels, H, W]\n",
    "\n",
    "        # Ensure the mask is a long (index) tensor\n",
    "        mask = mask.long()\n",
    "\n",
    "        # Mask shape\n",
    "        assert mask.ndim == 3  # [batch_size, H, W]\n",
    "\n",
    "        # Predict mask logits\n",
    "        logits_mask = self.forward(image)\n",
    "\n",
    "        assert (\n",
    "            logits_mask.shape[1] == self.number_of_classes\n",
    "        )  # [batch_size, number_of_classes, H, W]\n",
    "\n",
    "        # Ensure the logits mask is contiguous\n",
    "        logits_mask = logits_mask.contiguous()\n",
    "\n",
    "        # Compute loss using multi-class Dice loss (pass original mask, not one-hot encoded)\n",
    "        loss = self.loss_fn(logits_mask, mask)\n",
    "\n",
    "        # Apply softmax to get probabilities for multi-class segmentation\n",
    "        prob_mask = logits_mask.softmax(dim=1)\n",
    "\n",
    "        # Convert probabilities to predicted class labels\n",
    "        pred_mask = prob_mask.argmax(dim=1)\n",
    "\n",
    "        # Compute true positives, false positives, false negatives, and true negatives\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(\n",
    "            pred_mask, mask, mode=\"multiclass\", num_classes=self.number_of_classes\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        # Aggregate step metrics\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        # Per-image IoU and dataset IoU calculations\n",
    "        per_image_iou = smp.metrics.iou_score(\n",
    "            tp, fp, fn, tn, reduction=\"micro-imagewise\"\n",
    "        )\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "\n",
    "        metrics = {\n",
    "            f\"{stage}_per_image_iou\": per_image_iou,\n",
    "            f\"{stage}_dataset_iou\": dataset_iou,\n",
    "        }\n",
    "\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        train_loss_info = self.shared_step(batch, \"train\")\n",
    "        self.training_step_outputs.append(train_loss_info)\n",
    "        return train_loss_info\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.shared_epoch_end(self.training_step_outputs, \"train\")\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        valid_loss_info = self.shared_step(batch, \"valid\")\n",
    "        self.validation_step_outputs.append(valid_loss_info)\n",
    "        return valid_loss_info\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss_info = self.shared_step(batch, \"test\")\n",
    "        self.test_step_outputs.append(test_loss_info)\n",
    "        return test_loss_info\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.shared_epoch_end(self.test_step_outputs, \"test\")\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-4)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=1e-5)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "# Use SegFormer-B2 pretrained on ADE20K by setting encoder_name to \"mit_b2\"\n",
    "# and passing encoder_weights=\"ade20k\".\n",
    "model = CamVidModel(\"SegFormer\", \"mit_b0\", in_channels=3, out_classes=OUT_CLASSES, encoder_weights=\"imagenet\")\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) # Adjust num_workers as needed\n",
    "val_loader = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=4) # Adjust num_workers as needed\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, log_every_n_steps=1, fast_dev_run=True) # fast_dev_run=True will run only 1 batch for train and val\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_loader, # Pass the DataLoader\n",
    "    val_dataloaders=val_loader,     # Pass the DataLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:34:35.864340Z",
     "iopub.status.busy": "2024-08-23T11:34:35.864021Z",
     "iopub.status.idle": "2024-08-23T11:34:37.899992Z",
     "shell.execute_reply": "2024-08-23T11:34:37.898994Z",
     "shell.execute_reply.started": "2024-08-23T11:34:35.864305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # run validation dataset\n",
    "# valid_metrics = trainer.validate(model, dataloaders=valid_loader, verbose=False)\n",
    "# print(valid_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:34:37.901909Z",
     "iopub.status.busy": "2024-08-23T11:34:37.901489Z",
     "iopub.status.idle": "2024-08-23T11:34:41.364904Z",
     "shell.execute_reply": "2024-08-23T11:34:41.363771Z",
     "shell.execute_reply.started": "2024-08-23T11:34:37.901870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # run test dataset\n",
    "# test_metrics = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "# print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:34:41.366777Z",
     "iopub.status.busy": "2024-08-23T11:34:41.366427Z",
     "iopub.status.idle": "2024-08-23T11:35:08.321664Z",
     "shell.execute_reply": "2024-08-23T11:35:08.320487Z",
     "shell.execute_reply.started": "2024-08-23T11:34:41.366741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Fetch a batch from the test loader\n",
    "# images, masks = next(iter(test_loader))\n",
    "\n",
    "# # Switch the model to evaluation mode\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     logits = model(images)  # Get raw logits from the model\n",
    "\n",
    "# # Apply softmax to get class probabilities\n",
    "# # Shape: [batch_size, num_classes, H, W]\n",
    "\n",
    "# pr_masks = logits.softmax(dim=1)\n",
    "# # Convert class probabilities to predicted class labels\n",
    "# pr_masks = pr_masks.argmax(dim=1)  # Shape: [batch_size, H, W]\n",
    "\n",
    "# # Visualize a few samples (image, ground truth mask, and predicted mask)\n",
    "# for idx, (image, gt_mask, pr_mask) in enumerate(zip(images, masks, pr_masks)):\n",
    "#     if idx <= 4:  # Visualize first 5 samples\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "\n",
    "#         # Original Image\n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.imshow(\n",
    "#             image.cpu().numpy().transpose(1, 2, 0)\n",
    "#         )  # Convert CHW to HWC for plotting\n",
    "#         plt.title(\"Image\")\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#         # Ground Truth Mask\n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.imshow(gt_mask.cpu().numpy(), cmap=\"tab20\")  # Visualize ground truth mask\n",
    "#         plt.title(\"Ground truth\")\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#         # Predicted Mask\n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.imshow(pr_mask.cpu().numpy(), cmap=\"tab20\")  # Visualize predicted mask\n",
    "#         plt.title(\"Prediction\")\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#         # Show the figure\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf-checked",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
