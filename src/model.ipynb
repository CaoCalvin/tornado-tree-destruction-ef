{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:29:59.376434Z",
     "iopub.status.busy": "2024-08-23T11:29:59.376015Z",
     "iopub.status.idle": "2024-08-23T11:30:06.026067Z",
     "shell.execute_reply": "2024-08-23T11:30:06.024970Z",
     "shell.execute_reply.started": "2024-08-23T11:29:59.376395Z"
    },
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset as BaseDataset\n",
    "import torch.cuda as cuda\n",
    "import torch\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "import cv2 \n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch.optim import lr_scheduler\n",
    "import math\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. Please ensure you have a compatible GPU and CUDA installed.\")\n",
    "\n",
    "class Cls(Enum):\n",
    "    UPRIGHT = (0, \"#b7f2a6\")  # light green\n",
    "    FALLEN = (1, \"#c71933\")   # red\n",
    "    OTHER = (2, \"#ffcc33\")    # yellow\n",
    "\n",
    "    def __new__(cls, num, hex_color):\n",
    "        obj = object.__new__(cls)\n",
    "        obj._value_ = num\n",
    "        obj.hex_color = hex_color\n",
    "        obj.rgb_color = tuple(int(hex_color[i:i+2], 16) for i in (1, 3, 5))  # Convert hex to RGB\n",
    "        return obj\n",
    "    \n",
    "def resplit(dataset_path, train_frac=0.7, val_frac=0.2, test_frac=0.1, seed=SEED):\n",
    "    assert train_frac + val_frac + test_frac == 1.0, \"Fractions must sum to 1.\"\n",
    "\n",
    "    scenes = sorted(os.listdir(os.path.join(dataset_path, 'images')))\n",
    "    random.seed(seed)\n",
    "    random.shuffle(scenes)\n",
    "    \n",
    "    n = len(scenes)\n",
    "    n_train = int(n * train_frac)\n",
    "    # Use remaining scenes for both validation and test equally\n",
    "    remaining = n - n_train\n",
    "    half = remaining // 2  # ensure both splits have the same number of scenes\n",
    "    \n",
    "    train_scenes = scenes[:n_train]\n",
    "    val_scenes = scenes[n_train:n_train+half]\n",
    "    test_scenes = scenes[n_train+half:n_train+2*half]\n",
    "\n",
    "    os.makedirs(os.path.join(dataset_path, 'splits'), exist_ok=True)\n",
    "    for name, split in zip(['train', 'val', 'test'], [train_scenes, val_scenes, test_scenes]):\n",
    "        with open(os.path.join(dataset_path, 'splits', f'{name}.txt'), 'w') as f:\n",
    "            for scene in split:\n",
    "                f.write(f\"{scene}\\n\")\n",
    "\n",
    "class Dataset(BaseDataset):\n",
    "\n",
    "    def __init__(self, image_root, mask_root, split_file, transform=None):\n",
    "        self.background_class = Cls.UPRIGHT.value\n",
    "        assert os.path.exists(image_root), f\"Image root {image_root} does not exist.\"\n",
    "        assert os.path.exists(mask_root), f\"Mask root {mask_root} does not exist.\"\n",
    "        assert os.path.exists(split_file), f\"Split file {split_file} does not exist.\"\n",
    "        \n",
    "        self.image_root = image_root\n",
    "        self.mask_root = mask_root\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(split_file, 'r') as f:\n",
    "            self.scenes = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        self.samples = []\n",
    "        for scene in self.scenes:\n",
    "            image_dir = os.path.join(self.image_root, scene)\n",
    "            mask_dir = os.path.join(self.mask_root, scene)\n",
    "            \n",
    "            for fname in sorted(os.listdir(image_dir)):\n",
    "                if not fname.endswith(('.png', '.jpg', '.jpeg', '.tiff', '.tif')):\n",
    "                    continue\n",
    "                img_path = os.path.join(image_dir, fname)\n",
    "                name, ext = os.path.splitext(fname)\n",
    "                mask_path = os.path.join(mask_dir, f\"{name}_mask{ext}\")  \n",
    "                self.samples.append((img_path, mask_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self.samples[idx]\n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "        return image, mask\n",
    "    \n",
    "def get_training_augmentation():\n",
    "    return A.Compose([\n",
    "        A.Resize(height=256, width=256),  # Ensure consistent size\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.RandomCrop(height=256, width=256, p=0.5),\n",
    "        A.PadIfNeeded(min_height=256, min_width=256, border_mode=cv2.BORDER_CONSTANT, mask_value=Cls.OTHER.value, p=1.0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=20, val_shift_limit=15, p=0.3),\n",
    "        A.OneOf([\n",
    "            A.GaussianBlur(blur_limit=3, p=0.3),\n",
    "            A.MotionBlur(blur_limit=3, p=0.3),\n",
    "            A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.3),\n",
    "        ], p=0.3),\n",
    "        A.GaussNoise(p=0.2),\n",
    "        A.OneOf([\n",
    "            A.ElasticTransform(alpha=1, sigma=50, p=0.3, border_mode=cv2.BORDER_CONSTANT, mask_value=Cls.OTHER.value), \n",
    "            A.GridDistortion(num_steps=5, distort_limit=(-0.1, 0.1), p=0.3, border_mode=cv2.BORDER_CONSTANT, mask_value=Cls.OTHER.value),\n",
    "        ], p=0.2),\n",
    "        A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.05, rotate_limit=0, \n",
    "                           p=0.3, border_mode=cv2.BORDER_CONSTANT, mask_value=Cls.OTHER.value),\n",
    "        A.OneOf([\n",
    "            A.CLAHE(clip_limit=2, tile_grid_size=(8, 8), p=0.5),\n",
    "            A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "        ], p=0.3),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                    std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def visualize(**images):\n",
    "    \"\"\"Plot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, img) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(\" \".join(name.split(\"_\")).title())\n",
    "        if name == \"image\":\n",
    "            # Check if image is in CHW format; if so, convert it to HWC for plotting\n",
    "            if img.shape[0] == 3:\n",
    "                try:\n",
    "                    # If image is a torch tensor, use permute\n",
    "                    if isinstance(img, torch.Tensor):\n",
    "                        img = img.permute(1, 2, 0).cpu().numpy()\n",
    "                    else:\n",
    "                        img = img.transpose(1, 2, 0)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            plt.imshow(img)\n",
    "        else:\n",
    "            # Convert greyscale integer mask to RGB colored mask using the enum mapping\n",
    "            # Build a lookup table from enum values (0,1,2) to the corresponding RGB colors.\n",
    "            lut = np.array([\n",
    "                Cls.UPRIGHT.rgb_color,\n",
    "                Cls.FALLEN.rgb_color,\n",
    "                Cls.OTHER.rgb_color\n",
    "            ], dtype=np.uint8)\n",
    "            # Map the greyscale mask to an RGB image by indexing using the mask values.\n",
    "            color_mask = lut[img]\n",
    "            plt.imshow(color_mask)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class CamVidModel(pl.LightningModule):\n",
    "    def __init__(self, arch, encoder_name, in_channels, out_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch,\n",
    "            encoder_name=encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            classes=out_classes,\n",
    "            **kwargs,\n",
    "        )\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name)\n",
    "        self.number_of_classes = out_classes\n",
    "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1).to(device))\n",
    "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1).to(device))\n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.MULTICLASS_MODE, from_logits=True)\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, image):\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def shared_step(self, batch, stage, batch_idx):\n",
    "        image, mask = batch\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        assert image.ndim == 4, \"Expected image to have 4 dimensions, got shape \" + str(image.shape)\n",
    "        mask = mask.long()\n",
    "        assert mask.ndim == 3, \"Expected mask to have 3 dimensions, got shape \" + str(mask.shape)\n",
    "        logits_mask = self.forward(image)\n",
    "        assert logits_mask.shape[1] == self.number_of_classes, f\"Expected logits channel {self.number_of_classes}, got {logits_mask.shape[1]}\"\n",
    "        logits_mask = logits_mask.contiguous()\n",
    "        loss = self.loss_fn(logits_mask, mask)\n",
    "        prob_mask = logits_mask.softmax(dim=1)\n",
    "        pred_mask = prob_mask.argmax(dim=1)\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(\n",
    "            pred_mask, mask, mode=\"multiclass\", num_classes=self.number_of_classes\n",
    "        )\n",
    "\n",
    "        # MODIFICATION 2: Add the logging logic here\n",
    "        # Determine total batches and total images based on the current stage\n",
    "        if stage == 'train':\n",
    "            total_batches = self.trainer.num_training_batches\n",
    "            total_images = len(self.trainer.train_dataloader.dataset)\n",
    "        elif stage == 'valid':\n",
    "            # Assumes a single validation dataloader, as in your setup\n",
    "            total_batches = self.trainer.num_val_batches[0]\n",
    "            total_images = len(self.trainer.val_dataloaders.dataset)\n",
    "        else: # stage == 'test'\n",
    "            # Assumes a single test dataloader\n",
    "            total_batches = self.trainer.num_test_batches[0]\n",
    "            total_images = len(self.trainer.test_dataloaders.dataset)\n",
    "\n",
    "        # Calculate the number of images processed in this step\n",
    "        # We use batch_idx + 1 for human-readable 1-based indexing\n",
    "        current_batch_size = image.shape[0]\n",
    "        images_processed = (batch_idx * current_batch_size) + current_batch_size\n",
    "\n",
    "        # Log the formatted string to the console\n",
    "        print(\n",
    "            f\"Epoch {self.current_epoch} - {stage.title()} Step: {batch_idx + 1}/{total_batches} - Images: {images_processed}/{total_images}\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        metrics = {\n",
    "            f\"{stage}_per_image_iou\": per_image_iou,\n",
    "            f\"{stage}_dataset_iou\": dataset_iou,\n",
    "        }\n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        train_loss_info = self.shared_step(batch, \"train\", batch_idx)\n",
    "        self.training_step_outputs.append(train_loss_info)\n",
    "        return train_loss_info\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.shared_epoch_end(self.training_step_outputs, \"train\")\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        valid_loss_info = self.shared_step(batch, \"valid\", batch_idx)\n",
    "        self.validation_step_outputs.append(valid_loss_info)\n",
    "        return valid_loss_info\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        test_loss_info = self.shared_step(batch, \"test\", batch_idx)\n",
    "        self.test_step_outputs.append(test_loss_info)\n",
    "        return test_loss_info\n",
    "        \n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.shared_epoch_end(self.test_step_outputs, \"test\")\n",
    "        self.test_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = OPTIMIZER_TYPE(self.parameters(), lr=LEARNING_RATE)\n",
    "        scheduler = SCHEDULER_TYPE(optimizer, T_max=T_MAX, eta_min=ETA_MIN)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:30:06.039244Z",
     "iopub.status.busy": "2024-08-23T11:30:06.038840Z",
     "iopub.status.idle": "2024-08-23T11:30:06.054834Z",
     "shell.execute_reply": "2024-08-23T11:30:06.053760Z",
     "shell.execute_reply.started": "2024-08-23T11:30:06.039198Z"
    },
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dbutt7\\AppData\\Local\\miniconda3\\envs\\torch-env\\lib\\site-packages\\albumentations\\augmentations\\blur\\transforms.py:184: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "idx = random.randint(0, len(dataset_train) - 1)\n",
    "image, mask = dataset_train[idx] \n",
    "print(f\"Showing image {idx} of {len(dataset_train)}\")\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "visualize(image=image, mask=mask)\n",
    "\n",
    "\n",
    "base_path = os.path.join('..', 'dataset_processed')\n",
    "images_path = os.path.join(base_path, 'images')\n",
    "masks_path = os.path.join(base_path, 'masks')\n",
    "splits_path = os.path.join(base_path, 'splits')\n",
    "\n",
    "torch.set_float32_matmul_precision('medium') # TODO see if high is better or low doesn't make a difference\n",
    "\n",
    "resplit(base_path, train_frac=0.7, val_frac=0.15, test_frac=0.15)\n",
    "\n",
    "dataset_train = Dataset(\n",
    "    image_root=images_path,\n",
    "    mask_root=masks_path,\n",
    "    split_file=os.path.join(splits_path, 'train.txt'),\n",
    "    transform=get_training_augmentation()  \n",
    ")\n",
    "\n",
    "dataset_val = Dataset(\n",
    "    image_root=images_path,\n",
    "    mask_root=masks_path,\n",
    "    split_file=os.path.join(splits_path, 'val.txt'),\n",
    "    transform=get_validation_augmentation()  \n",
    ")\n",
    "\n",
    "dataset_test = Dataset(\n",
    "    image_root=images_path,\n",
    "    mask_root=masks_path,\n",
    "    split_file=os.path.join(splits_path, 'test.txt'),\n",
    "    transform=get_validation_augmentation()  \n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T11:30:07.615277Z",
     "iopub.status.busy": "2024-08-23T11:30:07.614938Z",
     "iopub.status.idle": "2024-08-23T11:30:07.638455Z",
     "shell.execute_reply": "2024-08-23T11:30:07.637474Z",
     "shell.execute_reply.started": "2024-08-23T11:30:07.615245Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcaocalvin\u001b[0m (\u001b[33mcalvincao\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250606_133659-fnfgr65g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/calvincao/treefall-tornado-rating/runs/fnfgr65g' target=\"_blank\">azure-thunder-4</a></strong> to <a href='https://wandb.ai/calvincao/treefall-tornado-rating' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/calvincao/treefall-tornado-rating' target=\"_blank\">https://wandb.ai/calvincao/treefall-tornado-rating</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/calvincao/treefall-tornado-rating/runs/fnfgr65g' target=\"_blank\">https://wandb.ai/calvincao/treefall-tornado-rating/runs/fnfgr65g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model   │ Segformer │  3.7 M │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ loss_fn │ DiceLoss  │      0 │ train │\n",
       "└───┴─────────┴───────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model   │ Segformer │  3.7 M │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ loss_fn │ DiceLoss  │      0 │ train │\n",
       "└───┴─────────┴───────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 395 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 3.3 M                                                                                        \n",
       "<span style=\"font-weight: bold\">Total params</span>: 3.7 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 14                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 194                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 395 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 3.3 M                                                                                        \n",
       "\u001b[1mTotal params\u001b[0m: 3.7 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 14                                                                         \n",
       "\u001b[1mModules in train mode\u001b[0m: 194                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249624f926d64118b15488908355621d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    # Some training hyperparameters TODO tune\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 16\n",
    "    T_MAX = EPOCHS * math.ceil(len(dataset_train) / BATCH_SIZE)\n",
    "    OUT_CLASSES = len(Cls)\n",
    "\n",
    "    # Optimizer and scheduler parameters\n",
    "    OPTIMIZER_TYPE = torch.optim.Adam\n",
    "    LEARNING_RATE = 2e-4\n",
    "    SCHEDULER_TYPE = lr_scheduler.CosineAnnealingLR\n",
    "    ETA_MIN = 1e-5\n",
    "\n",
    "    # Architecture and encoder parameters\n",
    "    ARCH = \"SegFormer\"\n",
    "    ENCODER_NAME = \"mit_b0\"\n",
    "    ENCODER_WEIGHTS = \"imagenet\"\n",
    "    \n",
    "    wandb_logger = WandbLogger(project=\"treefall-tornado-rating\", log_model=True)\n",
    "\n",
    "    model = CamVidModel(\n",
    "        ARCH,\n",
    "        ENCODER_NAME,\n",
    "        in_channels=3,\n",
    "        out_classes=OUT_CLASSES,\n",
    "        encoder_weights=ENCODER_WEIGHTS\n",
    "    ).to(device)\n",
    "    for param in model.model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    wandb_logger.experiment.config.update({\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"t_max\": T_MAX,\n",
    "        \"optimizer\": OPTIMIZER_TYPE.__name__,\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"scheduler\": SCHEDULER_TYPE.__name__,\n",
    "        \"min_lr\": ETA_MIN,\n",
    "        \"architecture\": ARCH,\n",
    "        \"encoder\": ENCODER_NAME,\n",
    "        \"encoder_weights\": ENCODER_WEIGHTS,\n",
    "        \"train_size\": len(dataset_train),\n",
    "        \"val_size\": len(dataset_val),\n",
    "        \"num_classes\": OUT_CLASSES,\n",
    "    })\n",
    "\n",
    "    # TODO: Tune number of workers based on system\n",
    "    train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=16, persistent_workers=True, pin_memory=True)\n",
    "    val_loader = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=16, persistent_workers=True, pin_memory=True)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        log_every_n_steps=1,\n",
    "        fast_dev_run=False,  # fast_dev_run=True will run only 1 batch for train and val\n",
    "        callbacks=[RichProgressBar()],\n",
    "        logger=wandb_logger \n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=val_loader,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
